{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning-based Highway Information Panel Reading\n",
        "\n",
        "## Fundamental implementation of an Optical Character Recognition (OCR) system using Python, OpenCV and scikit-learn\n",
        "\n",
        "    Original publish date: Jun, 2023\n",
        "    Version: v1.1\n",
        "    License: MIT\n",
        "    Author: Alejandro Asensio Pérez\n",
        "    Tags: Computer Vision, Optical Character Recognition, Machine Learning, Python\n",
        "\n",
        "**Disclaimer:** This work was created as part of my academic coursework at King Juan Carlos University. While I claim copyright for the selection and arrangement of the content, the copyright for the original materials used in this work is held by the university and the instructors, Victoria Ruiz and Ángel Sánchez.\n",
        "\n",
        "### Summary\n",
        "\n",
        "The notebook provides a foundation for building a system capable of automatically extracting information from highway panels.\n",
        "\n",
        "### Connect and Engage\n",
        "\n",
        "Please, feel free to comment on typos, propose improvements or expand the content."
      ],
      "metadata": {
        "id": "eLVlWH2uWYnK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1s5p0uid3hLLs9mc4QNs07nNxzLVA8aBX)\n",
        "*This figure illustrates the anticipated outcome of the character recognition process. On the left, we present an original road sign image captured in a real-world setting. On the right, the individual characters successfully detected and isolated by the classifier are displayed.*"
      ],
      "metadata": {
        "id": "oVSoej5vAtvv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Notice:** This notebook is designed to run on Google Colab. However, it can be run locally using a virtual python environment with the following Setup."
      ],
      "metadata": {
        "id": "z068BM1eCnmJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Helper Functions\n",
        "\n",
        "This section lays the groundwork for the OCR system by:\n",
        "\n",
        "* Importing libraries: Includes necessary libraries for image processing, machine learning, and visualization.\n",
        "* Defining helper functions: Establishes custom functions to streamline tasks like plotting confusion matrices and preprocessing images for character recognition.\n",
        "\n",
        "These preparations ensure a smooth and organized development process for the subsequent character recognition."
      ],
      "metadata": {
        "id": "OZ6sUKjFuTae"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mx8u8Q0alx6Y"
      },
      "outputs": [],
      "source": [
        "# Import standard libraries\n",
        "import string\n",
        "import numpy as np\n",
        "import glob\n",
        "import cv2\n",
        "import imutils\n",
        "\n",
        "# Import libraries for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import machine learning libraries\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Import Google Colab patches for OpenCV\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Import Google Drive integration\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to plot confusion matrix\n",
        "def plot_confusion_matrix(cm, title='Confusion matrix', cmap='Blues'):\n",
        "  \"\"\"\n",
        "  Visualizes the performance of a classification model using a confusion matrix.\n",
        "\n",
        "  This function generates a color-coded heatmap that displays the counts of true\n",
        "  positive, true negative, false positive, and false negative predictions,\n",
        "  providing insights into the model's accuracy and error patterns.\n",
        "\n",
        "  Args:\n",
        "      cm: The confusion matrix to plot.\n",
        "      title: The title of the plot.\n",
        "      cmap: The color map to use.\n",
        "  \"\"\"\n",
        "  plt.imshow(cm, interpolation='nearest', cmap=plt.cm.get_cmap(cmap))\n",
        "  plt.title(title)\n",
        "  tick_marks = np.arange(cm.shape[0])\n",
        "  plt.xticks(tick_marks, range(cm.shape[0]))\n",
        "  plt.yticks(tick_marks, range(cm.shape[0]))\n",
        "  plt.tight_layout()\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "\n",
        "  # Add annotations\n",
        "  ax = plt.gca()\n",
        "  for x in range(cm.shape[1]):\n",
        "    for y in range(cm.shape[0]):\n",
        "      ax.annotate(str(cm[y, x]), xy=(y, x), horizontalalignment='center',\n",
        "                      verticalalignment='center')"
      ],
      "metadata": {
        "id": "qGQESWDRURpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process and img for OCR train\n",
        "def process_image(img, size = 25):\n",
        "  \"\"\"\n",
        "  Prepares an image for character recognition by resizing and binarizing it.\n",
        "\n",
        "  This function transforms the input image into a format suitable for\n",
        "  character recognition algorithms by:\n",
        "\n",
        "  1. Resizing it to a standardized size for consistency.\n",
        "  2. Converting it to a binary image (black and white) using Otsu's thresholding\n",
        "     to enhance contrast and simplify feature extraction.\n",
        "  3. Flattening the image into a 1D array for compatibility with machine learning models.\n",
        "\n",
        "  Args:\n",
        "      img: The image to process.\n",
        "\n",
        "  Returns:\n",
        "      The processed image.\n",
        "  \"\"\"\n",
        "  img_resized = cv2.resize(img, (size, size))\n",
        "  _, img_binary = cv2.threshold(img_resized, 0, 1, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
        "  img_reshaped = img_binary.reshape(1, size * size)\n",
        "\n",
        "  return img_reshaped"
      ],
      "metadata": {
        "id": "03IP5HTDcdFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OCR Classifier\n",
        "\n",
        "This section focuses on building and evaluating a character classifier to recognize individual characters from images. It explores different machine learning algorithms, including Linear Discriminant Analysis (LDA), K-Nearest Neighbors (KNN), and Random Forest, to determine the most effective approach for character recognition. The goal is to train a model that can accurately predict the character present in an image based on its features.\n",
        "\n",
        "This classifier forms the core component of the OCR system, enabling the recognition of characters within highway information panels."
      ],
      "metadata": {
        "id": "wbVAWAj_uaKb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing Training and Validation Data\n",
        "\n",
        "This section sets up the paths to the training and validation datasets. The `train_path` dictionary organizes the paths for different character categories (numbers, lowercase, uppercase) stored in Google Drive.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1z8_9r2em8HdGSgA8dy0BQDeztisVNBN3)\n",
        "*This figure showcases a diverse set of images used to train the text classifier.*\n",
        "\n",
        "Please note that the actual data is not available due to copyright related issues. You can find similar examples in the MNIST dataset. Remember that the performance of a machine learning model is heavily dependent on the quality and representativeness of its training data.\n",
        "\n",
        "> Note: If running this notebook locally, replace the Google Drive paths with local file paths using the `os` module."
      ],
      "metadata": {
        "id": "lAZ_S-bkJ_d-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = {\n",
        "  'numbers': ('/content/drive/MyDrive/ocr-system/d/nums', string.digits),\n",
        "  'lowercase': ('/content/drive/MyDrive/ocr-system/d/lowers', string.ascii_lowercase),\n",
        "  'uppercase': ('/content/drive/MyDrive/ocr-system/d/uppers', string.ascii_uppercase)\n",
        "}\n",
        "\n",
        "validation_path = '/content/drive/MyDrive/ocr-system/d/validation'\n",
        "\n",
        "panel_path = '/content/drive/MyDrive/ocr-system/d/panels'\n",
        "\n",
        "train_values = []\n",
        "train_tags = []\n",
        "\n",
        "validation_values = []\n",
        "validation_tags = []"
      ],
      "metadata": {
        "id": "N_Jh2q7GjUAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading and Preprocessing Training and Validation Data\n",
        "\n",
        "This section handles the crucial task of preparing both training and validation datasets for model training and evaluation. It involves loading images, applying preprocessing steps, and organizing the data for effective use.\n",
        "\n",
        "**Training Data**\n",
        "\n",
        "The training images for numbers, lowercase, and uppercase characters are loaded and preprocessed. This includes:\n",
        "\n",
        "1. Resizing: Each image is scaled down to 25x25 pixels to standardize input size.\n",
        "2. Binarization: A threshold is applied to convert grayscale images into binary (black and white) for simplified feature extraction.\n",
        "3. Flattening: The binary images are converted into 1-dimensional arrays (1-row binary matrices) for compatibility with machine learning algorithms.\n",
        "4. Labeling: Each image is associated with its corresponding character label.\n",
        "\n",
        "**Validation Data**\n",
        "\n",
        "Similarly, validation images are loaded and preprocessed using the same steps. This validation set serves as an independent dataset to assess the performance of the trained model on unseen data, providing a more realistic estimate of its generalization capabilities.\n",
        "\n",
        "> Note: For this validation, the number '2' is used as the target character.\n",
        "\n",
        "By carefully preparing both training and validation data, we ensure that the model is trained on a representative dataset and evaluated on its ability to generalize to new examples."
      ],
      "metadata": {
        "id": "qbLYMqx1MHZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for category, (path, chars) in train_path.items():\n",
        "  print(f'Loading {category}: [', end='')\n",
        "  for char in chars:\n",
        "    current_path = path + '/' + char\n",
        "    for img_path in glob.glob(current_path + '/*.png'):\n",
        "      img_raw = cv2.imread(img_path, 0)\n",
        "      train_values.append(process_image(img_raw)[0])\n",
        "      train_tags.append(char)\n",
        "    print(f'{char}', end='')\n",
        "  print('] Done')"
      ],
      "metadata": {
        "id": "HPPJU3WdOIdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Loading validation: [', end='')\n",
        "for img_path in glob.glob(validation_path + '/' + '*.png'):\n",
        "  img_raw = cv2.imread(img_path, 0)\n",
        "  validation_values.append(process_image(img_raw)[0])\n",
        "  validation_tags.append('2')  # Label all validation images as '2' in these use-case\n",
        "  print(f'.', end='')\n",
        "print('] Done')"
      ],
      "metadata": {
        "id": "kxvCXY5aO5WL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing Data for Model Training\n",
        "\n",
        "This section formats the preprocessed data for use with machine learning algorithms. Currently, the code utilizes all available samples for training, meaning there's no separate testing set (`x_test`) allocated. As a result, model evaluation relies solely on the validation set (`validation_tags`).\n",
        "\n",
        "It's important to note that using all data for training can lead to overfitting, where the model performs well on the training data but poorly on unseen data. Ideally, a portion of the data should be reserved for testing to assess the model's generalization capabilities."
      ],
      "metadata": {
        "id": "ekevvCFsSWCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, _, y_train, _ = train_test_split(train_values, train_tags, test_size=1, random_state=0)"
      ],
      "metadata": {
        "id": "iTKLBqxqhawE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the current setup uses raw pixel values, further preprocessing can sometimes improve model performance. For instance, standardizing the data can be beneficial for certain algorithms.\n",
        "\n",
        "**Standardization?**\n",
        "\n",
        "Standardization of a dataset is often necessary for machine learning estimators because many of them assume that the features follow a standard normal distribution (Gaussian with 0 mean and unit variance). If the features don't roughly adhere to this distribution, the model's performance might suffer.\n",
        "\n",
        "You can achieve standardization using `StandardScaler` as shown below:\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(train_values)\n",
        "X_test = sc.transform(train_tags)\n",
        "```"
      ],
      "metadata": {
        "id": "hKgMqsl0SrDH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training and Evaluation\n",
        "\n",
        "This section delves into the core of the character recognition process, involving:\n",
        "\n",
        "* Model training: Utilizes the preprocessed training data to train various machine learning models (LDA, KNN, Random Forest) for character classification.\n",
        "* Model evaluation: Assesses the performance of the trained models using validation data and metrics like confusion matrices and accuracy scores.\n",
        "\n",
        "This systematic approach aims to identify the most effective model for accurately recognizing characters within highway information panels."
      ],
      "metadata": {
        "id": "BuICKEMEbDJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training the LDA Model\n",
        "\n",
        "We begin by training a Linear Discriminant Analysis (LDA) model for character recognition. In this initial step, we utilize all available training data to fit the LDA model. This means that the model learns to distinguish between characters based on the patterns and features present in the entire training dataset.\n",
        "\n",
        "**Dimensionality Reduction with LDA**\n",
        "\n",
        "A key aspect of LDA is its ability to reduce the dimensionality of feature vectors. The LinearDiscriminantAnalysis.fit() method finds the optimal LDA projection matrix. We then use the transform method of the fitted LDA object to project the original matrix into a lower-dimensional space, resulting in the Column-Row (CR) Matrix. This dimensionality reduction can improve model efficiency and potentially enhance performance by focusing on the most discriminative features."
      ],
      "metadata": {
        "id": "s_YPBWSRbOVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda = LinearDiscriminantAnalysis(n_components=1)\n",
        "\n",
        "X_train_lda = lda.fit_transform(X_train, y_train)"
      ],
      "metadata": {
        "id": "yyOt0PwPSra5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LDA values prediction and Validation\n",
        "\n",
        "After training the LDA model and preprocessing the validation images, we use the trained model to predict the character present in each validation image. The `lda.predict(validation_values)` function takes the preprocessed validation images as input and generates a list of predicted character labels. These predictions represent the model's best guess for the character in each image based on the patterns it learned from the training data.\n",
        "\n",
        "By comparing these predictions to the actual character labels (which are all '2' in this case), we can evaluate the accuracy of the LDA model on unseen data.\n",
        "\n",
        "Evaluating LDA Performance\n",
        "\n",
        "To gain a deeper understanding of the LDA model's performance, we generate a confusion matrix and calculate the accuracy score.\n",
        "\n",
        "* Confusion Matrix: The confusion matrix provides a detailed breakdown of the model's predictions, showing the counts of true positives, true negatives, false positives, and false negatives. This helps identify specific areas where the model might be making errors.\n",
        "* Accuracy Score: The accuracy score represents the overall percentage of correct predictions made by the model on the validation set. It provides a simple and intuitive metric to assess the model's overall performance.\n",
        "\n",
        "By analyzing these evaluation metrics, we can gain insights into the strengths and weaknesses of the LDA model and make informed decisions about potential improvements or alternative approaches."
      ],
      "metadata": {
        "id": "CX-xjeTFe0vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Value prediction\n",
        "y_pred_lda = lda.predict(validation_values)\n",
        "print(y_pred_lda)\n",
        "\n",
        "# Validation\n",
        "cm = confusion_matrix(validation_tags, y_pred_lda)\n",
        "plot_confusion_matrix(cm)\n",
        "print('Accuracy: ' + str(accuracy_score(validation_tags, y_pred_lda)))"
      ],
      "metadata": {
        "id": "RsSH-azwu60F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluating a KNN Classifier with 5 Neighbors\n",
        "\n",
        "We assess the performance of a KNN classifier with k=5 using the `.score` method on the validation set. This provides a direct measure of the model's accuracy on unseen data, helping us gauge its generalization capabilities."
      ],
      "metadata": {
        "id": "8ZRPcvpMfUE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "knn = KNeighborsClassifier(5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Validation\n",
        "score = knn.score(validation_values, validation_tags)\n",
        "print('Accuracy: ' + str(score))"
      ],
      "metadata": {
        "id": "i08Y81Vw2cN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exploring Random Forest Classification with Depth 15 and 10 Estimators\n",
        "\n",
        "We employ a Random Forest classifier with a maximum depth of 15 and 10 decision trees (estimators). Due to the inherent randomness in the algorithm's tree construction, the performance metrics (such as accuracy) may vary slightly between different executions.\n"
      ],
      "metadata": {
        "id": "xXr7jAiPgHV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "rf = RandomForestClassifier(max_depth=15, n_estimators=10, max_features=1)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Validation\n",
        "score = rf.score(validation_values, validation_tags)\n",
        "print('Accuracy: ' + str(score))"
      ],
      "metadata": {
        "id": "-uTMgBtnAn7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Note: KNN achieves the highest accuracy, due to being a multivariate approach."
      ],
      "metadata": {
        "id": "R_j-pfyJWJAq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automatic Road Panel Reading\n",
        "\n",
        "This section delves into the development of a system capable of automatically identifying and interpreting road panels. By leveraging image processing techniques and machine learning algorithms, we aim to extract crucial information from these panels, such as warnings and directions."
      ],
      "metadata": {
        "id": "mPB9pf8XwBt9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1ppweByOCBLyfy6o_Jn8oE-scVFbBbFbY)\n",
        "*This photograph captures a typical roadside scene featuring two distinct signs. Our focus lies on the sign with high contrast between the font color and background.*"
      ],
      "metadata": {
        "id": "hp4VVKKvWONC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract and Recognize Characters from Images\n",
        "\n",
        "* Contour detection: We utilize thresholding and OpenCV's contour detection utilities to identify potential character regions within the image.\n",
        "* Size filtering: Only contours with dimensions similar to characters are considered for further processing.\n",
        "* Character recognition: Extracted regions are fed into a character recognition algorithm for prediction.\n",
        "* Result visualization: Recognized characters are overlaid onto the original image for visualization.\n",
        "\n",
        "**Text Line Detection**\n",
        "\n",
        "For improved text analysis, consider incorporating an inline/outline technique estimator like RANSAC (Random Sample Consensus) to identify and align text lines within the image."
      ],
      "metadata": {
        "id": "Ok9IZUHIuoP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = []\n",
        "\n",
        "print('Loading cropped images', end='')\n",
        "for img in glob.glob(panel_path + '/' + '*.png'):\n",
        "  img_raw = cv2.imread(img)\n",
        "\n",
        "  # Turn the image gray, apply a binary threshold and finf contours\n",
        "  img_gray = cv2.cvtColor(img_raw, cv2.COLOR_BGR2GRAY)\n",
        "  _, img_binary = cv2.threshold(img_gray, 127, 255, cv2.THRESH_BINARY)\n",
        "  contours, _ = cv2.findContours(img_binary, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "  # For every contour found\n",
        "  for cnt in contours:\n",
        "    x,y,w,h = cv2.boundingRect(cnt)\n",
        "\n",
        "    # If it has a character size\n",
        "    if (w>5 and h>5) and (w<50 and h<50):\n",
        "      # Crop the character section\n",
        "      char = img_binary[y:y+h,x:x+w]\n",
        "\n",
        "      # Resize keeping the aspect ratio\n",
        "      char_resized = imutils.resize(char, width=50, inter=cv2.INTER_AREA)\n",
        "\n",
        "      # Canvas where to place the character so that it has the same format as the training data\n",
        "      char_canvas = np.zeros((150, 150), dtype=np.uint8)\n",
        "\n",
        "      # Calculate padding\n",
        "      x_i = int((150 - 50) / 2)\n",
        "      x_f = x_i + 50\n",
        "      y_i = int((150 - char_resized.shape[0]) / 2)\n",
        "      y_f = y_i + char_resized.shape[0]\n",
        "\n",
        "      try:\n",
        "        char_canvas[y_i:y_f, x_i:x_f] = char_resized\n",
        "      except:\n",
        "        pass  # Character exceeds the bounds\n",
        "\n",
        "      # Final processing\n",
        "      char_processed = process_image(char_canvas)\n",
        "      features.append(char_processed[0])\n",
        "\n",
        "      # Character prediction with the proposed algorithms\n",
        "      char_lda = lda.predict(char_processed)\n",
        "      char_knn = knn.predict(char_processed)\n",
        "      char_rf = rf.predict(char_processed)\n",
        "\n",
        "      # Write the character into the image at the beginning of the contour\n",
        "      # KNN proved to be the best classifier, so it is the one used to print the character\n",
        "      cv2.putText(img_raw, (char_knn[0]), (x, y), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                  .5, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "  # Final image with all predicted characters\n",
        "  cv2_imshow(img_final)"
      ],
      "metadata": {
        "id": "Tp2kJw-QwGwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1n0O4YMA6Jy3PaoQ1ZS6PKqh5TLFBTs3q)\n",
        "*This figure depicts the entire panel recognition process, including inline character detection. From left to right, binary thresholding separates characters, followed by detection and alignment of character contours. Finally, the rightmost section displays the predicted characters overlaid onto the panel, revealing the extracted text.*"
      ],
      "metadata": {
        "id": "5CnEs3QlZZd9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Thanks for following along!** 🙂 If you have implemented a similar approach or have ideas for improvement, I'd love to see your code.\n",
        "\n",
        "Also, if you find the work helpful and would like to cite them, you can use the following bibtex:\n",
        "\n",
        "```bibtex\n",
        "@misc{aaseper2024mlhipr,\n",
        "   title        = {{Machine Learning-based Highway Information Panel Reading}},\n",
        "   author       = {Alejandro Asensio},\n",
        "   year         = 2024,\n",
        "   howpublished = {\\url{https://drive.google.com/file/d/13Vh_FzU2B65amu_QCTyWuon2wn0YgBSY/view?usp=sharing}}\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "I_gHFzRseYMH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "© Copyright 2024 Alejandro Asensio Pérez."
      ],
      "metadata": {
        "id": "0Kfucankc3Fp"
      }
    }
  ]
}
